import os
import pandas as pd
import numpy as np
import yfinance as yf
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# ==================== PARAMS ====================
TICKERS = [
    "AAPL","MSFT","GOOGL","AMZN","META","NVDA","TSLA","BRK-B","JPM","BAC","WFC","MS","GS",
    "V","MA","PYPL","KO","PEP","PG","JNJ","UNH","MRK","PFE","T","VZ","CMCSA","DIS","NFLX",
    "INTC","AMD","IBM","ORCL","CRM","COST","HD","WMT","TGT","LOW","NKE","MCD","SBUX","CAT",
    "BA","GE","LMT","RTX","XOM","CVX","COP","SLB"
]

START = "2014-01-01"
INTERVAL = "1mo"
FORECAST_HORIZON = 3       # Pr√©dire les rendements sur 3 mois
TRAIN_END_DATE = "2020-01-01"  # Split temporel
TOP_N_STOCKS = 10          # Nombre de stocks dans le portefeuille ML
REBALANCE_FREQ = 3         # Rebalancer tous les N mois
COSTS_BPS = 20             # Co√ªts de transaction

out_dir = "./capstone_output"
os.makedirs(out_dir, exist_ok=True)
ts = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")

print("=" * 80)
print("CAPSTONE PROJECT: VALUE vs GROWTH - ML CROSS-SECTIONAL")
print("=" * 80)
print(f"‚öôÔ∏è  Train period: {START} ‚Üí {TRAIN_END_DATE}")
print(f"‚öôÔ∏è  Test period: {TRAIN_END_DATE} ‚Üí present")
print(f"‚öôÔ∏è  Rebalancing every {REBALANCE_FREQ} months")
print("=" * 80)

# ==================== SECTION 1: DOWNLOAD PRICES ====================
print("\n[1/9] Downloading price data...")
px = yf.download(TICKERS, start=START, interval=INTERVAL,
                 auto_adjust=True, progress=False)["Close"]
px = px.dropna(how="all")
rets = np.log(px / px.shift(1)).dropna()
print(f"‚úì Downloaded {len(px)} periods for {len(px.columns)} tickers")

# ==================== SECTION 2: FETCH FUNDAMENTALS ====================
print("\n[2/9] Fetching current fundamentals & building historical proxy...")

def fetch_current_fundamentals(ticker):
    """Fetch current fundamentals as proxy (limitation: pas de donn√©es historiques via yfinance)"""
    try:
        info = yf.Ticker(ticker).info
    except Exception:
        info = {}
    
    mcap = info.get("marketCap")
    fcf = info.get("freeCashflow")
    p_fcf = (mcap / fcf) if (mcap and fcf and fcf != 0) else None
    
    return {
        "pe": info.get("trailingPE"),
        "pb": info.get("priceToBook"),
        "ps": info.get("priceToSalesTrailing12Months"),
        "ev_ebitda": info.get("enterpriseToEbitda"),
        "p_fcf": p_fcf,
        "revenue_growth": info.get("revenueGrowth"),
        "eps_growth": info.get("earningsQuarterlyGrowth"),
        "sector": info.get("sector"),
    }

current_fund = {ticker: fetch_current_fundamentals(ticker) for ticker in TICKERS}
print(f"‚úì Fetched current fundamentals for {len(current_fund)} stocks")
print("‚ö†Ô∏è  LIMITATION: Using current fundamentals as proxy for historical data")
print("   In production: use historical fundamental data from specialized providers")

# ==================== SECTION 3: BUILD CROSS-SECTIONAL DATASET ====================
print("\n[3/9] Building cross-sectional dataset (date, ticker) ‚Üí features, target...")

def build_cross_sectional_dataset(prices, returns, fundamentals_dict, horizon=3):
    """
    Construit un dataset panel avec:
    - index: MultiIndex (date, ticker)
    - features: P/E, P/B, P/S, etc. (simul√©s comme constants, id√©alement historiques)
    - target: rendement futur sur horizon (somme des log-returns)
    """
    data_rows = []
    
    for date in returns.index[:-horizon]:  # On exclut les derni√®res dates sans futur
        for ticker in returns.columns:
            if ticker not in fundamentals_dict:
                continue
            
            fund = fundamentals_dict[ticker]
            
            # Target: rendement cumul√© sur les prochains N mois
            future_rets = returns.loc[date:, ticker].iloc[1:horizon+1]
            if len(future_rets) < horizon:
                continue
            target = future_rets.sum()  # Log returns ‚Üí somme = cumul
            
            row = {
                "date": date,
                "ticker": ticker,
                "pe": fund.get("pe"),
                "pb": fund.get("pb"),
                "ps": fund.get("ps"),
                "ev_ebitda": fund.get("ev_ebitda"),
                "p_fcf": fund.get("p_fcf"),
                "revenue_growth": fund.get("revenue_growth"),
                "eps_growth": fund.get("eps_growth"),
                "target": target,
            }
            data_rows.append(row)
    
    df = pd.DataFrame(data_rows)
    df = df.set_index(["date", "ticker"])
    return df

dataset = build_cross_sectional_dataset(px, rets, current_fund, FORECAST_HORIZON)
dataset = dataset.dropna()
print(f"‚úì Created dataset with {len(dataset)} observations (date √ó ticker pairs)")
print(f"   Features: {[c for c in dataset.columns if c != 'target']}")
print(f"   Sample size: {dataset.index.get_level_values('date').nunique()} dates √ó ~{len(TICKERS)} tickers")

# ==================== SECTION 4: TRAIN-TEST SPLIT ====================
print("\n[4/9] Splitting data temporally...")

train_dates = dataset.index.get_level_values("date") < TRAIN_END_DATE
train_data = dataset[train_dates]
test_data = dataset[~train_dates]

X_train = train_data.drop("target", axis=1)
y_train = train_data["target"]
X_test = test_data.drop("target", axis=1)
y_test = test_data["target"]

print(f"‚úì Train set: {len(X_train)} observations "
      f"({X_train.index.get_level_values('date').min()} ‚Üí {X_train.index.get_level_values('date').max()})")
print(f"‚úì Test set:  {len(X_test)} observations "
      f"({X_test.index.get_level_values('date').min()} ‚Üí {X_test.index.get_level_values('date').max()})")

# Standardisation
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ==================== SECTION 5: TRAIN ML MODEL ====================
print("\n[5/9] Training single cross-sectional model...")

models = {
    "Ridge": Ridge(alpha=1.0),
    "RandomForest": RandomForestRegressor(
        n_estimators=100, max_depth=6, random_state=42, n_jobs=-1
    ),
    "GradientBoosting": GradientBoostingRegressor(
        n_estimators=100, max_depth=4, random_state=42
    ),
}

best_model = None
best_score = -np.inf
best_name = None
feat_imp = None  # sera rempli si le mod√®le a des importances

for name, model in models.items():
    print(f"   Training {name}...")
    model.fit(X_train_scaled, y_train)
    train_score = model.score(X_train_scaled, y_train)
    test_score = model.score(X_test_scaled, y_test)
    
    print(f"   ‚Üí Train R¬≤: {train_score:.4f} | Test R¬≤: {test_score:.4f}")
    
    if test_score > best_score:
        best_score = test_score
        best_model = model
        best_name = name

print(f"\n‚úì Best model: {best_name} (Test R¬≤ = {best_score:.4f})")

# Feature importance si disponible
if hasattr(best_model, "feature_importances_"):
    feat_imp = pd.Series(
        best_model.feature_importances_, index=X_train.columns
    ).sort_values(ascending=False)
    print("\nüìä Feature Importances (Top 5):")
    print(feat_imp.head())

# Pr√©dictions
y_pred = best_model.predict(X_test_scaled)
test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"‚úì Test RMSE: {test_rmse:.6f}")

# ==================== SECTION 6: BUILD DYNAMIC ML PORTFOLIO ====================
print("\n[6/9] Building dynamic ML portfolio with periodic rebalancing...")

# Reconstituer les pr√©dictions par date
test_preds = pd.Series(y_pred, index=y_test.index, name="prediction")
test_df = test_data.copy()
test_df["prediction"] = test_preds

# Dates de rebalancing
test_dates = sorted(test_df.index.get_level_values("date").unique())
rebalance_dates = test_dates[::REBALANCE_FREQ]

print(f"‚úì Rebalancing on {len(rebalance_dates)} dates (every {REBALANCE_FREQ} months)")

weights_ml = {}
current_holdings = []

for rb_date in rebalance_dates:
    date_slice = test_df.xs(rb_date, level="date")
    date_preds = date_slice.sort_values("prediction", ascending=False)
    
    # Top N stocks
    top_stocks = date_preds.head(TOP_N_STOCKS).index.tolist()
    if len(top_stocks) == 0:
        continue
    w = pd.Series(1.0 / len(top_stocks), index=top_stocks)
    
    # Appliquer ces poids jusqu'au prochain rebalancing
    idx_rb = rebalance_dates.index(rb_date)
    if idx_rb + 1 < len(rebalance_dates):
        end_date = rebalance_dates[idx_rb + 1]
        period_dates = [d for d in test_dates if rb_date <= d < end_date]
    else:
        period_dates = [d for d in test_dates if d >= rb_date]
    
    for d in period_dates:
        weights_ml[d] = w
    
    current_holdings.append({
        "date": rb_date,
        "holdings": top_stocks,
        "avg_prediction": date_preds.head(TOP_N_STOCKS)["prediction"].mean(),
    })

print(f"‚úì Portfolio constructed with dynamic rebalancing")

# ==================== SECTION 7: BACKTEST ====================
print("\n[7/9] Running backtest with transaction costs...")

def backtest_dynamic(weights_dict, returns, costs_bps=20):
    """Backtest avec rebalancing dynamique et co√ªts de transaction."""
    dates_bt = sorted(weights_dict.keys())
    equity = []
    prev_w = None
    
    for d in dates_bt:
        if d not in returns.index:
            continue
        w = weights_dict[d].reindex(returns.columns).fillna(0.0)
        r = float((w * returns.loc[d]).sum())
        
        # Co√ªts de transaction
        if prev_w is None:
            tc = (costs_bps / 10000.0) * w.abs().sum()  # entr√©e initiale
        else:
            turnover = (w - prev_w).abs().sum()
            tc = (costs_bps / 10000.0) * turnover
        
        equity.append(r - tc)
        prev_w = w
    
    logret = pd.Series(equity, index=sorted(weights_dict.keys())[:len(equity)])
    return np.exp(logret.cumsum())

# Sous-ensemble des rendements pour la p√©riode de test
test_start = test_dates[0]
test_rets = rets.loc[test_start:]

# Benchmarks Value / Growth statiques bas√©s sur fundamentals actuels
fund_df = pd.DataFrame(current_fund).T
value_cols = [c for c in ["pe", "pb", "ps", "ev_ebitda", "p_fcf"] if c in fund_df.columns]
inv = fund_df[value_cols].copy()
for c in value_cols:
    inv[c] = 1.0 / fund_df[c].where(fund_df[c] > 0)
inv = inv.replace([np.inf, -np.inf], np.nan).fillna(inv.median(numeric_only=True))
Z = (inv - inv.mean()) / inv.std(ddof=0)
score_value = Z.mean(axis=1)

valid = score_value.dropna()
dec = pd.qcut(
    valid.rank(method="first", ascending=False),
    10, labels=False, duplicates="drop"
) + 1
deciles = pd.Series(index=score_value.index, dtype="Int64")
deciles.loc[dec.index] = dec

val_names = deciles.index[deciles == 1]
grw_names = deciles.index[deciles == 10]

w_val = pd.Series(1 / len(val_names), index=val_names) if len(val_names) else pd.Series(dtype=float)
w_grw = pd.Series(1 / len(grw_names), index=grw_names) if len(grw_names) else pd.Series(dtype=float)
w_mkt = pd.Series(1 / len(test_rets.columns), index=test_rets.columns)

# Poids constants pour benchmarks sur les dates de test
weights_val = {d: w_val for d in test_dates}
weights_grw = {d: w_grw for d in test_dates}
weights_mkt = {d: w_mkt for d in test_dates}

# Backtests
eq_ml = backtest_dynamic(weights_ml, test_rets, COSTS_BPS)
eq_val = backtest_dynamic(weights_val, test_rets, COSTS_BPS)
eq_grw = backtest_dynamic(weights_grw, test_rets, COSTS_BPS)
eq_mkt = backtest_dynamic(weights_mkt, test_rets, 0.0)

print(f"‚úì Backtest complete on {len(eq_ml)} periods")

# ==================== SECTION 8: VISUALIZATION ====================
print("\n[8/9] Generating visualizations...")

def _base1(s):
    return s / s.iloc[0] if len(s) else s

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. Equity curves
ax = axes[0, 0]
_base1(eq_val).rename("VALUE (Q1)").plot(ax=ax, linewidth=2, alpha=0.8)
_base1(eq_grw).rename("GROWTH (Q10)").plot(ax=ax, linewidth=2, alpha=0.8)
_base1(eq_ml).rename("ML DYNAMIC (Top10)").plot(ax=ax, linewidth=2.5, linestyle='-')
_base1(eq_mkt).rename("MARKET (EW)").plot(ax=ax, linewidth=1.5, linestyle="--", alpha=0.7)
ax.set_title(f"Out-of-Sample Performance ({TRAIN_END_DATE} onwards)")
ax.set_ylabel("Cumulative Value (base=1)")
ax.legend()
ax.grid(alpha=0.3)

# 2. Drawdowns
ax = axes[0, 1]
for eq, name, style in [
    (eq_ml, "ML DYNAMIC", "-"),
    (eq_val, "VALUE", "--"),
    (eq_grw, "GROWTH", "--"),
    (eq_mkt, "MARKET", ":"),
]:
    dd = (eq / eq.cummax() - 1) * 100
    dd.plot(ax=ax, label=name, alpha=0.7, linestyle=style)
ax.set_title("Drawdown Comparison (%)")
ax.set_ylabel("Drawdown %")
ax.legend()
ax.grid(alpha=0.3)

# 3. Feature importance (si disponible)
ax = axes[1, 0]
if feat_imp is not None:
    feat_imp.plot(kind="barh", ax=ax)
    ax.set_title(f"Feature Importance ({best_name})")
    ax.set_xlabel("Importance")
    ax.grid(alpha=0.3)
else:
    ax.axis("off")

# 4. Distribution des pr√©dictions
ax = axes[1, 1]
test_df["prediction"].hist(bins=50, ax=ax, edgecolor="black", alpha=0.7)
ax.axvline(0, color="red", linestyle="--", linewidth=2)
ax.set_title("Distribution of ML Predictions (Test Set)")
ax.set_xlabel("Predicted 3-Month Return")
ax.set_ylabel("Frequency")
ax.grid(alpha=0.3)

plt.tight_layout()
plot_path = os.path.join(out_dir, f"ml_cross_sectional_{ts}.png")
plt.savefig(plot_path, dpi=150)
plt.close()
print(f"‚úì Saved plots ‚Üí {plot_path}")

# ==================== SECTION 9: PERFORMANCE SUMMARY ====================
def perf_stats(eq):
    logr = np.log(eq / eq.shift(1)).dropna()
    mu = float(logr.mean() * 12)
    sig = float(logr.std(ddof=0) * (12 ** 0.5))
    sharpe = mu / (sig + 1e-12)
    mdd = float((eq / eq.cummax() - 1).min())
    final_value = float(eq.iloc[-1])
    return {
        "CAGR": f"{mu * 100:.2f}%",
        "Volatility": f"{sig * 100:.2f}%",
        "Sharpe": round(sharpe, 3),
        "MaxDD": f"{mdd * 100:.2f}%",
        "Final Value": round(final_value, 3),
    }

print("\n" + "=" * 80)
print("OUT-OF-SAMPLE PERFORMANCE SUMMARY (Post-2020)")
print("=" * 80)
print(f"ML DYNAMIC:     {perf_stats(eq_ml)}")
print(f"VALUE (Q1):     {perf_stats(eq_val)}")
print(f"GROWTH (Q10):   {perf_stats(eq_grw)}")
print(f"MARKET (EW):    {perf_stats(eq_mkt)}")

# Turnover analysis
print("\n" + "=" * 80)
print("PORTFOLIO TURNOVER ANALYSIS")
print("=" * 80)
print(f"Number of rebalances: {len(rebalance_dates)}")
print(f"Average holdings per rebalance: {TOP_N_STOCKS}")
print(f"Rebalancing frequency: every {REBALANCE_FREQ} months")

overlaps = []
for i in range(len(current_holdings) - 1):
    curr = set(current_holdings[i]["holdings"])
    nxt = set(current_holdings[i + 1]["holdings"])
    overlap = len(curr.intersection(nxt)) / TOP_N_STOCKS
    overlaps.append(overlap)

if overlaps:
    print(f"Average holdings overlap: {np.mean(overlaps) * 100:.1f}%")
    print(f"Implied turnover per rebalance: {(1 - np.mean(overlaps)) * 100:.1f}%")

# Save equity curves
results_df = pd.DataFrame({
    "ML_DYNAMIC": eq_ml,
    "VALUE_Q1": eq_val,
    "GROWTH_Q10": eq_grw,
    "MARKET_EW": eq_mkt,
})
results_path = os.path.join(out_dir, f"equity_curves_{ts}.csv")
results_df.to_csv(results_path)

# Save holdings
holdings_df = pd.DataFrame([
    {"date": h["date"], "ticker": t, "rank": i + 1}
    for h in current_holdings
    for i, t in enumerate(h["holdings"])
])
holdings_path = os.path.join(out_dir, f"ml_holdings_{ts}.csv")
holdings_df.to_csv(holdings_path, index=False)

print("\n" + "=" * 80)
print("KEY IMPROVEMENTS IN THIS VERSION:")
print("=" * 80)
print("‚úì Cross-sectional dataset (all dates √ó tickers together)")
print("‚úì Single model trained on full panel data")
print("‚úì Proper temporal train/test split (train pre-2020, test post-2020)")
print("‚úì Dynamic portfolio rebalancing based on fresh predictions")
print("‚úì Transaction costs applied at each rebalance")
print("‚úì Out-of-sample performance metrics")
print("\n‚ö†Ô∏è  LIMITATION: Still using current fundamentals as proxy")
print("   Next step: Integrate historical fundamental data API")
print("=" * 80)
print(f"\nAll files saved to: {os.path.abspath(out_dir)}")
print("=" * 80)
print("‚úì Cross-sectional ML project complete!")
